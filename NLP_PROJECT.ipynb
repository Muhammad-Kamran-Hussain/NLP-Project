{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcuqoAn-X2nH"
      },
      "outputs": [],
      "source": [
        "# !pip install ffmpeg-python\n",
        "# !pip install faster-whisper\n",
        "# !pip install nltk scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wj6NeObCYEfP"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "# !pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xww_911YG_s"
      },
      "outputs": [],
      "source": [
        "# !pip install googletrans==4.0.0-rc1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgYHkGBgYHdY",
        "outputId": "3d69f8c3-fe26-4ccb-9a14-cdc0288328a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import ffmpeg\n",
        "from IPython.display import Audio\n",
        "from faster_whisper import WhisperModel\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from googletrans import Translator\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C11ZVzBXYd_2"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNekXTB9YfF9"
      },
      "outputs": [],
      "source": [
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Initialize the Whisper model\n",
        "model_size = \"medium\"\n",
        "model = WhisperModel(model_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2jfY9syZrIy"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHCG6GKzY4W3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize translator\n",
        "translator = Translator()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agBC2tDpFKBH"
      },
      "outputs": [],
      "source": [
        "# Initialize transformer-based sentiment analysis model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "model_sentiment = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "sentiment_pipeline = pipeline('sentiment-analysis', model=model_sentiment, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN_d9qn5FOyT"
      },
      "outputs": [],
      "source": [
        "# Function to convert video to audio\n",
        "def video_to_audio(video_path):\n",
        "    videofilename = video_path.split('/')[-1]\n",
        "    audiofilename = videofilename.replace(\".mp4\", '.mp3')\n",
        "    input_stream = ffmpeg.input(video_path)\n",
        "    audio = input_stream.audio\n",
        "    output_stream = ffmpeg.output(audio, audiofilename)\n",
        "    output_stream = ffmpeg.overwrite_output(output_stream)\n",
        "    ffmpeg.run(output_stream)\n",
        "    return audiofilename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iIO-mQuFTkQ"
      },
      "outputs": [],
      "source": [
        "# Function to transcribe audio\n",
        "def transcribe_audio(audio_path):\n",
        "    segments, info = model.transcribe(audio_path, word_timestamps=True)\n",
        "    if not segments:\n",
        "        raise ValueError(\"Transcription failed, segments are None or empty.\")\n",
        "    transcription_string = \" \".join([word.word for segment in segments for word in segment.words])\n",
        "    return transcription_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xn8rzlwFaxm"
      },
      "outputs": [],
      "source": [
        "# Function to translate text\n",
        "def translate_text(text, target_language='en'):\n",
        "    translated = translator.translate(text, dest=target_language)\n",
        "    return translated.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHO0AlFbFe4S"
      },
      "outputs": [],
      "source": [
        "# Function to perform topic modeling\n",
        "def get_topics(text, num_topics=1, num_words=5):\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "    X = vectorizer.fit_transform([text])\n",
        "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n",
        "    lda.fit(X)\n",
        "    words = vectorizer.get_feature_names_out()\n",
        "    topics = []\n",
        "    for topic_idx, topic in enumerate(lda.components_):\n",
        "        topic_words = [words[i] for i in topic.argsort()[:-num_words - 1:-1]]\n",
        "        topics.append(\" \".join(topic_words))\n",
        "    return topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k1rYfdlFkRh"
      },
      "outputs": [],
      "source": [
        "# Function to analyze sentiment using VADER and Transformers\n",
        "def analyze_sentiment(text):\n",
        "    # VADER sentiment analysis\n",
        "    vader_scores = sid.polarity_scores(text)\n",
        "\n",
        "    # Transformer-based sentiment analysis\n",
        "    transformer_scores = sentiment_pipeline(text)\n",
        "    transformer_sentiment = transformer_scores[0]['label'].lower()\n",
        "\n",
        "    # Combining both sentiment scores\n",
        "    combined_scores = {\n",
        "        'vader': vader_scores,\n",
        "        'transformer': transformer_sentiment\n",
        "    }\n",
        "    return combined_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leRl2mBiFqT6",
        "outputId": "14481f67-b53f-47d1-9fb2-f9fb809206db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the path to your news video (or 'done' to finish): /content/Iranâ€™s President and Foreign Minister feared dead in helicopter crash _ BBC News.mp4\n",
            "\n",
            "Detected language: en\n",
            "\n",
            "Transcription:\n",
            " A  major  search  operation  is  underway  after  a  helicopter  carrying  Iran's  president  and  its  foreign  minister  crashed  in  the  northwest  of  the  country  in  thick  fog.  President  Ebrahim  Raisi  had  been  returning  from  a  visit  to  the  border  with  neighboring\n",
            "\n",
            "--- Analysis Results ---\n",
            "Topics: president visit iran carrying country\n",
            "Sentiment (VADER): {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "Sentiment (Transformer): 4 stars\n",
            "------------------------\n",
            "\n",
            "Enter the path to your news video (or 'done' to finish): done\n"
          ]
        }
      ],
      "source": [
        "# Main loop to process multiple videos\n",
        "all_transcriptions = []\n",
        "all_sentiments = []\n",
        "\n",
        "while True:\n",
        "    video_path = input(\"Enter the path to your news video (or 'done' to finish): \")\n",
        "    if video_path.lower() == 'done':\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        audio_path = video_to_audio(video_path)\n",
        "        transcription = transcribe_audio(audio_path)\n",
        "\n",
        "        # Detect language\n",
        "        detected_language = translator.detect(transcription).lang\n",
        "        print(f\"\\nDetected language: {detected_language}\")\n",
        "\n",
        "        if detected_language != 'en':\n",
        "            transcription_translated = translate_text(transcription, target_language='en')\n",
        "            print(f\"\\nTranscription (translated to English):\\n{transcription_translated}\")\n",
        "        else:\n",
        "            transcription_translated = transcription\n",
        "            print(f\"\\nTranscription:\\n{transcription_translated}\")\n",
        "\n",
        "        topic = get_topics(transcription_translated)\n",
        "        sentiment = analyze_sentiment(transcription_translated)\n",
        "\n",
        "        all_transcriptions.append(transcription_translated)\n",
        "        all_sentiments.append(sentiment)\n",
        "\n",
        "        print(\"\\n--- Analysis Results ---\")\n",
        "        print(f\"Topics: {', '.join(topic)}\")\n",
        "        print(f\"Sentiment (VADER): {sentiment['vader']}\")\n",
        "        print(f\"Sentiment (Transformer): {sentiment['transformer']}\")\n",
        "        print(\"------------------------\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {video_path}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck_Vw_oCFyoI",
        "outputId": "fe0f03a1-a739-4923-9c4f-42a23bb0699d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Overall sentiment for today's news (VADER):\n",
            "Negative: 0.00\n",
            "Neutral: 1.00\n",
            "Positive: 0.00\n",
            "Compound: 0.00\n",
            "\n",
            "Overall sentiment: Neutral\n"
          ]
        }
      ],
      "source": [
        "# Summarize overall sentiment using VADER\n",
        "overall_sentiment_vader = {\n",
        "    'neg': sum([s['vader']['neg'] for s in all_sentiments]) / len(all_sentiments),\n",
        "    'neu': sum([s['vader']['neu'] for s in all_sentiments]) / len(all_sentiments),\n",
        "    'pos': sum([s['vader']['pos'] for s in all_sentiments]) / len(all_sentiments),\n",
        "    'compound': sum([s['vader']['compound'] for s in all_sentiments]) / len(all_sentiments),\n",
        "}\n",
        "\n",
        "print(\"\\nOverall sentiment for today's news (VADER):\")\n",
        "print(f\"Negative: {overall_sentiment_vader['neg']:.2f}\")\n",
        "print(f\"Neutral: {overall_sentiment_vader['neu']:.2f}\")\n",
        "print(f\"Positive: {overall_sentiment_vader['pos']:.2f}\")\n",
        "print(f\"Compound: {overall_sentiment_vader['compound']:.2f}\")\n",
        "\n",
        "if (overall_sentiment_vader['pos'] > overall_sentiment_vader['neg'] and\n",
        "    overall_sentiment_vader['pos'] > overall_sentiment_vader['neu']):\n",
        "    overall_sentiment = \"Positive\"\n",
        "elif (overall_sentiment_vader['neg'] > overall_sentiment_vader['pos'] and\n",
        "      overall_sentiment_vader['neg'] > overall_sentiment_vader['neu']):\n",
        "    overall_sentiment = \"Negative\"\n",
        "else:\n",
        "    overall_sentiment = \"Neutral\"\n",
        "\n",
        "print(f\"\\nOverall sentiment: {overall_sentiment}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR1YKllsF1KF",
        "outputId": "a5b6ef04-0e89-4b96-df2f-13f8d549a56e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Overall sentiment for today's news (Transformer):\n",
            "Positive: 40.00%\n",
            "Negative: 20.00%\n",
            "Neutral: 40.00%\n",
            "\n",
            "Overall sentiment: Neutral\n"
          ]
        }
      ],
      "source": [
        "# Sample data (replace with your actual data)\n",
        "all_sentiments = [\n",
        "    {'transformer': 'positive'},\n",
        "    {'transformer': 'negative'},\n",
        "    {'transformer': 'neutral'},\n",
        "    {'transformer': 'positive'},\n",
        "    {'transformer': 'neutral'}\n",
        "]\n",
        "\n",
        "# Summarize overall sentiment using Transformer\n",
        "transformer_sentiments = [s['transformer'] for s in all_sentiments]\n",
        "positive_count = transformer_sentiments.count('positive')\n",
        "negative_count = transformer_sentiments.count('negative')\n",
        "neutral_count = transformer_sentiments.count('neutral')\n",
        "total = len(transformer_sentiments)\n",
        "\n",
        "print(\"\\nOverall sentiment for today's news (Transformer):\")\n",
        "print(f\"Positive: {positive_count / total * 100:.2f}%\")\n",
        "print(f\"Negative: {negative_count / total * 100:.2f}%\")\n",
        "print(f\"Neutral: {neutral_count / total * 100:.2f}%\")\n",
        "\n",
        "# Determine the overall sentiment based on the highest count\n",
        "if positive_count > negative_count and positive_count > neutral_count:\n",
        "    overall_sentiment = \"Positive\"\n",
        "elif negative_count > positive_count and negative_count > neutral_count:\n",
        "    overall_sentiment = \"Negative\"\n",
        "else:\n",
        "    overall_sentiment = \"Neutral\"\n",
        "\n",
        "print(f\"\\nOverall sentiment: {overall_sentiment}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
